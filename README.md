[![Actions Status](https://github.com/trueb2/hetnetdb/workflows/Build%20and%20Test/badge.svg)](https://github.com/trueb2/hetnetdb/actions)


# HETNETDB

Hetnetdb is a database that is intended to be extremely flexible and easy to use. In hetnetdb (het, short for heteorogenous, and net, short for networked), the goal is to provide easy access to data in heterogeneous storage platforms and networks via SQL queries. Goals of the database include supporting CSV and JSON stored on multiple storage platforms including local, server, edge, and streaming. To achieve this goal, there will be agent support for browser, iOS, android, linux, and macOS. (Not windows, we specifically don't want to support windows users!). Computational operations will be executed by opaque HTTP endpoints for extreme ease of use.


## Concessions: IO, Crunch, RAM
In supporting edge peripherals as units in the execution graph, we open the door for supporting extremely large networks of devices. This can make crunching numbers generated by puny devices very easy and tracable, but it can also increase overall query execution time and create a long tail for individual stages of execution time.

With such limitations out in the open, the graph of execution must account for drastic differences in disk, network, and processing in order to provide low-latency query performance for various topologies. Improving this performance is a secondary goal; design decisions will be made to emphasize usability first!

## General Architecture and Nomenclature

Before defining terms, let's define a teir list: Good(TG), Bad(TB), and Ugly(TU). This teir list will be used to refer to the grade of capabilities of an attribute. These rankings indicate order(s) of magnitude difference. For example, an edge device communicating over BLE would have networking capabilities on the Ugly Teir or TU for short. Your smartphone might get TB for networking, and a server in the datacenter will get TG.


* Query Server: This is a host in the cloud. This runs the HTTP Server with endpoints for submitting query requests, requesting execution, etc. Generally, a Query Server should be TG.
* Agent: This is a host of executors. Agents are heterogenous in hetnetdb with sets of capabilities all over the teir list. Agents should not share resources with other agents, unless they have TU rankings. Agents may manage executors to retry or balance workloads, but when they fail to find an acceptable executor configuration, the query fails.
* Executor: The actual data manipulation happens inside of an executor. They respect their resource limitations and do their best to complete a job. They are fully independent from each other. They produce either results or maybe recoverable errors.
* parser: The interface for accessing data is SQL. The parser will turn your query into an error message or an execution graph.
* Execution Nodes: These are the high level todo list for the query. The Query Server keeps track of agents and delegates nodes to agents as it traverses the execution graph
* Execution Graph: The execution graph is the parsed plan for executing Execution Nodes. Every query has one graph pending finalization as error or results.


## Topologies

Obviously, there are some ambitious goals that aren't going to be completed any time soon.

### v0 target topologies

1. Query Server + (_Enter Generic Client_) Agent: A user has CSV/JSON they uploaded to the (_website_, _app_). They run the SQL query in the (_browser_, _phone_).
1. Query Server + Local Agent: A user has a directory of CSV/JSON. They run a SQL query against that data without upload.
1. Query Server + Local Agent + Remote Streaming Agent: A user has a directory of CSV/JSON. A Remote Streaming Agent is measuring data from an sensor and uploading to Query Server. Remote Streaming Agent's data is processed on Query Server + Local Agent.

### v1 target topologies

1. Query Server Service + Multi-tenant Agent Pools + Elastic Storage: A user submits a query to Query Service. Data flows from agent pools and elastic storage. Compute happens in Query Service and Agent Pools.
1. Query Server Service + Elastic Storage + Local Agent + Remote Streaming Agent: A user sumbits a query to Query Service. Data flows from elastic storage and remote streaming agents. New data from streaming agent is persisted in elastic storage and/or local agent.
1. Query Server Service + Edge Agents: Botnets can compute and store too
1. Query Server Service + Data redistribution: A user can move data to desired agents or have the service decide the best place

### v3 target topologies

1. Query Server Service + Mutli-tenant Agent Pools + Edge Agents: Run a social media platform. pay agents with ad revenue.


## Performance Goals

There are a couple important workloads considering target datasets will include various attributes including:
1. Time filter
1. Geo filter
1. Id filter
1. Capture then filter
1. Shuffle data distribution

Realistically, with the target architectures for v0, the best we can hope for is sub-second latency. With on-the-fly data parsing and HTTP servers at every hop of the execution graph, 500ms query execution time would be amazing. As agents enter the pool and indexes get more complicated new goals for new workloads will be established.


## Development

1. Install: cargo, libpq, diesel_cli (with postgres), systemfd, cargo-watch
2. Build/Test: `cargo build` or `cargo test`
3. Run dev server: `systemfd --no-pid -s http::6969 -- cargo watch -x run`
4. Run prod server: `cargo run --release`

## Example Usage

1. Install httpie
2. Submit the query to the endpoint piecewise or wholesale:

```
jwtrueb@jbmp hetnetdb % echo '{ "text": "SELECT count(*) from agents" }' | http post :6969/query/submit
HTTP/1.1 200 OK
content-length: 87
content-type: application/json
date: Sat, 14 Nov 2020 21:23:58 GMT

{
    "records": [
        {
            "columns": [
                {
                    "i64": 42
                }
            ],
            "ready": {
                "dt_utc": "2020-11-14T21:23:58.730786Z"
            }
        }
    ]
}
```

# Feature Roadmap and Wish List

While in the early stages of the project, this TODO list will hold the temporary feature roadmap.

- [x] Create an HTTP Server to route all actions through. All requests will perform message passing via HTTP requests and manipulate shared state by transacting with postgres.
- [x] Create routes for submitting select queries:
    - [x] To try to parse a SQL query
    - [x] To optimize a SQL query
    - [x] To execute an optimized SQL query
    - [x] To submit an unchecked SQL query and wait for results
- [ ] Inflate an optimized query into an execution graph
- [ ] Create execution nodes for data _
    - [ ] Data filtering: WHERE
    - [ ] Data grouping: GROUP BY
    - [ ] Data ordering: ORDER BY
    - [ ] Data functions:
        - [ ] COUNT(*)
        - [ ] COUNT()
        - [ ] COUNT(DISTINCT)
        - [ ] min
        - [ ] max
        - [ ] avg
        - [ ] ?stddev
- [ ] Create execution nodes for data load
    - [ ] Data limiting: LIMIT
    - [ ] Data offset: OFFSET
- [ ] Create routes for data load with schema enforcement
    - [x] To upload CSV to be cached
    - [ ] To parse CSV that is cached
    - [ ] To stream CSV into cached table
    - [ ] To register S3 configs to download the data (via HTTP request)
    - [ ] To register agent configs to process data locally (requires agency CLI/daemon services)
- [ ] Create agency CLI
    - [ ] Register capabilities
    - [ ] Heartbeat system load
    - [ ] Mark registration in computational group
    - [ ] Mark registration in storage group
    - [ ] Process compute events from Query Server Service
    - [ ] Process storage events from Query Server Service
- [ ] Configuring a monitoring dashboard
    - [ ] TICK stack
    - [ ] LogDNA event tracing for inidividual queries
- [ ] Create an execution graph visualization tool

# Improvement Wish List

- [ ] Create execution cost models and benchmarks
- [ ] Improve query optimization
- [ ] Improve execution graph inflation
- [ ] Add JSON support for CSV whereever CSV is referenced
    - [ ] Pick a faster serde format too
- [ ] Re-route workloads on heartbeat system load events
- [ ] Switch to a different parser that supports
    - [ ] Common Table Expressions
    - [ ] Window Functions
    - [ ] Reasonably Abritrary Syntax Expansions
- [ ] Run a benchmark on about 1B rows and/or 100GB uncompressed CSV
- [ ] Run an agency CLI service on something that produces rows by streaming from an edge device
- [ ] Provide reliability mechanisms for tracking ingestion of partially ingested data (after endpoint before rest).

# Milestone Interactions

## First Count Star

Data was
* Not persisted
* Generated by `seq 1 100000`
* Uploaded 100 times in parallel
* Only allowed to come from 1 table
* Expected to be traversed line by line to count

```
jwtrueb@jbmp hetnetdb % for i in `seq 1 100`; do http --multipart POST :6969/tables/upload/1 'Authorization: Bearer zKpze8PrHL0RfEoZwTeFKCrzL56RprSwJRm1hFp6KwTOfInwAzW8btLHuiMtfD12' csv@./sequence.csv & ; done
jwtrueb@jbmp hetnetdb % echo '{ "text": "select count(*) from simple" }' | http POST :6969/query/submit 'Authorization: Bearer zKpze8PrHL0RfEoZwTeFKCrzL56RprSwJRm1hFp6KwTOfInwAzW8btLHuiMtfD12'
HTTP/1.1 200 OK
content-length: 92
content-type: application/json
date: Mon, 16 Nov 2020 04:36:29 GMT

{
    "records": [
        {
            "columns": [
                {
                    "i64": 1000000
                }
            ],
            "ready": {
                "dt_utc": "2020-11-16T04:36:29.126917Z"
            }
        }
    ]
}
```
